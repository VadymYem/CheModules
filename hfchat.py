# meta developer: @blazeftg, @wsinfo
# üîí Licensed under the GNU AGPLv3
# üåê https://www.gnu.org/licenses/agpl-3.0.html

import logging
import requests

from .. import loader, utils

logger = logging.getLogger(__name__)

@loader.tds
class HuggingChatMod(loader.Module):
    """–£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π —á–∞—Ç –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é –ª–µ–≥–∫–∏—Ö –º–æ–¥–µ–ª–µ–π HuggingFace"""

    strings = {
        "name": "HuggingChat",
        "no_args": "‚ùå <b>–í–≤–µ–¥—ñ—Ç—å –∑–∞–ø–∏—Ç:</b> <code>{}{} {}</code>",
        "no_token": "‚ùå <b>–¢–æ–∫–µ–Ω –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ! –î–æ–¥–∞–π—Ç–µ:</b> <code>.cfg huggingchat</code>",
        "asking": "üîÑ <b>–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ...</b>",
        "answer": "ü§ñ <b>–í—ñ–¥–ø–æ–≤—ñ–¥—å {}:</b>\n{}\n\nüí¨ <b>–ó–∞–ø–∏—Ç:</b> {}",
        "api_error": "üö® <b>–ü–æ–º–∏–ª–∫–∞ API:</b> {}",
        "model_error": "‚ö†Ô∏è <b>–ü—Ä–æ–±–ª–µ–º–∞ –∑ –º–æ–¥–µ–ª–ª—é:</b>\n{}",
        "suggest_models": "üè∑ <b>–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω—ñ –º–æ–¥–µ–ª—ñ:</b>\n{}",
        "hf_models": (
            "<b>ü¶æ –†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω—ñ –º–æ–¥–µ–ª—ñ –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:</b>\n\n"
            "‚îå <code>meta-llama/Meta-Llama-3-8B-Instruct</code>\n"
            "‚îú <code>mistralai/Mistral-7B-Instruct-v0.2</code>\n"
            "‚îú <code>HuggingFaceH4/zephyr-7b-beta</code>\n"
            "‚îú <code>tiiuae/falcon-7b-instruct</code>\n"
            "‚îî <code>EleutherAI/gpt-neo-2.7B</code>\n\n"
            "üìå <i>–î–ª—è –∑–º—ñ–Ω–∏ –º–æ–¥–µ–ª—ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π</i> <code>.cfg huggingchat </code> –≤ `default_model` –Ω–∞–∑–≤–∞ –º–æ–¥–µ–ª—ñ —è–∫–∞ –Ω–∞–º —Ç—Ä–µ–±–∞"
        ),
    }
    
    strings_ru = {
    "name": "HuggingChat",
    "no_args": "‚ùå <b>–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å:</b> <code>{}{} {}</code>",
    "no_token": "‚ùå <b>–¢–æ–∫–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω! –î–æ–±–∞–≤—å—Ç–µ:</b> <code>.cfg huggingchat</code>",
    "asking": "üîÑ <b>–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞...</b>",
    "answer": "ü§ñ <b>–û—Ç–≤–µ—Ç {}:</b>\n{}\n\nüí¨ <b>–ó–∞–ø—Ä–æ—Å:</b> {}",
    "api_error": "üö® <b>–û—à–∏–±–∫–∞ API:</b> {}",
    "model_error": "‚ö†Ô∏è <b>–ü—Ä–æ–±–ª–µ–º–∞ —Å –º–æ–¥–µ–ª—å—é:</b>\n{}",
    "suggest_models": "üè∑ <b>–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏:</b>\n{}",
    "hf_models": (
        "<b>ü¶æ –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:</b>\n\n"
        "‚îå <code>meta-llama/Meta-Llama-3-8B-Instruct</code>\n"
        "‚îú <code>mistralai/Mistral-7B-Instruct-v0.2</code>\n"
        "‚îú <code>HuggingFaceH4/zephyr-7b-beta</code>\n"
        "‚îú <code>tiiuae/falcon-7b-instruct</code>\n"
        "‚îî <code>EleutherAI/gpt-neo-2.7B</code>\n\n"
        "üìå <i>–î–ª—è —Å–º–µ–Ω—ã –º–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π</i> <code>.cfg huggingchat </code> –≤ `default_model` –Ω–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏, –∫–æ—Ç–æ—Ä–∞—è –Ω–∞–º –Ω—É–∂–Ω–∞"
    ),
}

    strings_en = {
    "name": "HuggingChat",
    "no_args": "‚ùå <b>Enter a query:</b> <code>{}{} {}</code>",
    "no_token": "‚ùå <b>Token not found! Add:</b> <code>.cfg huggingchat</code>",
    "asking": "üîÑ <b>Generating response...</b>",
    "answer": "ü§ñ <b>Response {}:</b>\n{}\n\nüí¨ <b>Query:</b> {}",
    "api_error": "üö® <b>API Error:</b> {}",
    "model_error": "‚ö†Ô∏è <b>Model issue:</b>\n{}",
    "suggest_models": "üè∑ <b>Recommended models:</b>\n{}",
    "hf_models": (
        "<b>ü¶æ Recommended models for use:</b>\n\n"
        "‚îå <code>meta-llama/Meta-Llama-3-8B-Instruct</code>\n"
        "‚îú <code>mistralai/Mistral-7B-Instruct-v0.2</code>\n"
        "‚îú <code>HuggingFaceH4/zephyr-7b-beta</code>\n"
        "‚îú <code>tiiuae/falcon-7b-instruct</code>\n"
        "‚îî <code>EleutherAI/gpt-neo-2.7B</code>\n\n"
        "üìå <i>To change the model, use</i> <code>.cfg huggingchat </code> in `default_model` with the desired model name"
    ),
}


    def __init__(self):
        self.config = loader.ModuleConfig(
            loader.ConfigValue(
                "hf_api_key",
                None,
                "üîë –¢–æ–∫–µ–Ω HuggingFace (WRITE access required)",
                validator=loader.validators.Hidden(loader.validators.String())
            ),
            loader.ConfigValue(
                "default_model",
                "HuggingFaceH4/zephyr-7b-beta",
                "ü¶æ Default model (HuggingFace ID)",
            ),
            loader.ConfigValue(
                "max_new_tokens",
                512,
                "üìè Max. Tokens",
                validator=loader.validators.Integer(minimum=100, maximum=1024)
            ),
            loader.ConfigValue(
                "temperature",
                0.7,
                "üé® Creativity (0.1-1.0)",
                validator=loader.validators.Float(minimum=0.1, maximum=1.0)
            ),
        )
        
        self.recommended_models = [
            "meta-llama/Meta-Llama-3-8B-Instruct",
            "mistralai/Mistral-7B-Instruct-v0.2",
            "HuggingFaceH4/zephyr-7b-beta",
            "tiiuae/falcon-7b-instruct",
            "EleutherAI/gpt-neo-2.7B"
        ]

    async def client_ready(self, client, db):
        self.client = client
        self.db = db

    def _format_prompt(self, model: str, question: str) -> str:
        if "llama" in model.lower():
            return f"[INST] {question} [/INST]"
        elif "zephyr" in model.lower():
            return f"<|user|>\n{question}</s>\n<|assistant|>"
        elif "mistral" in model.lower():
            return f"[INST] {question} [/INST]"
        elif "gpt-neo" in model.lower():
            return f"{question}\n"  # –í–ò–ü–†–ê–í–õ–ï–ù–û –í–Ü–î–°–¢–£–ü–ò
        else:
            return question

    @loader.command()
    async def hfmodels(self, message):
        """–ü–æ–∫–∞–∑–∞—Ç–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω—ñ –º–æ–¥–µ–ª—ñ"""
        await utils.answer(message, self.strings["hf_models"])

    @loader.command()
    async def hfchat(self, message):
        """–ó–∞–¥–∞—Ç–∏ –ø–∏—Ç–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ"""
        question = utils.get_args_raw(message)
        if not question:
            return await utils.answer(message, self.strings["no_args"].format(
                self.get_prefix(), "hfchat", "[–∑–∞–ø–∏—Ç]"
            ))

        if not self.config["hf_api_key"]:
            return await utils.answer(message, self.strings["no_token"].format(self.get_prefix()))

        await utils.answer(message, self.strings["asking"])

        headers = {
            "Authorization": f"Bearer {self.config['hf_api_key']}",
            "Content-Type": "application/json",
        }

        current_model = self.config["default_model"]
        payload = {
            "inputs": self._format_prompt(current_model, question),
            "parameters": {
                "max_new_tokens": self.config["max_new_tokens"],
                "temperature": self.config["temperature"],
                "return_full_text": False
            }
        }

        try:
            response = await utils.run_sync(
                requests.post,
                url=f"https://api-inference.huggingface.co/models/{current_model}",
                headers=headers,
                json=payload
            )

            if response.status_code == 200:
                result = response.json()
                if isinstance(result, list) and "generated_text" in result[0]:
                    answer = result[0]["generated_text"].strip()
                    return await utils.answer(
                        message,
                        self.strings["answer"].format(
                            current_model.split('/')[-1],
                            answer,
                            question
                        )
                    )
                return await utils.answer(message, self.strings["api_error"].format("–ù–µ–≤—ñ—Ä–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ"))

            error_data = response.json()
            error_msg = f"{response.status_code} - {error_data.get('error', '–ù–µ–≤—ñ–¥–æ–º–∞ –ø–æ–º–∏–ª–∫–∞')}"
            
            if response.status_code == 503:
                suggestions = "\n".join([f"‚Ä¢ <code>{m}</code>" for m in self.recommended_models])
                error_msg += f"\n\n{self.strings['suggest_models'].format(suggestions)}"
            
            return await utils.answer(
                message,
                self.strings["model_error"].format(error_msg)
            )

        except Exception as e:
            logger.exception("API error")
            return await utils.answer(
                message,
                self.strings["api_error"].format(str(e))
            )