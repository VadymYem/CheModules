# meta developer: @blazeftg, @wsinfo
# üîí Licensed under the GNU AGPLv3
# üåê https://www.gnu.org/licenses/agpl-3.0.html

import logging
import requests

from .. import loader, utils

logger = logging.getLogger(__name__)

@loader.tds
class HuggingChatMod(loader.Module):
    """–£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π —á–∞—Ç –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é –ª–µ–≥–∫–∏—Ö –º–æ–¥–µ–ª–µ–π HuggingFace"""

    strings = {
        "name": "HuggingChat",
        "no_args": "‚ùå <b>–í–≤–µ–¥—ñ—Ç—å –∑–∞–ø–∏—Ç:</b> üìå <code>{}{} {}</code>",
        "no_token": "‚ùå <b>–¢–æ–∫–µ–Ω –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ! –î–æ–¥–∞–π—Ç–µ:</b> üìå <code>{}cfg huggingchat</code>",
        "asking": "üîÑ <b>–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ...</b>",
        "answer": "ü§ñ <b>–í—ñ–¥–ø–æ–≤—ñ–¥—å {}:</b>\n{}\n\nüí¨ <b>–ó–∞–ø–∏—Ç:</b> {}",
        "api_error": "üö® <b>–ü–æ–º–∏–ª–∫–∞ API:</b> {}",
        "model_error": "‚ö†Ô∏è <b>–ü—Ä–æ–±–ª–µ–º–∞ –∑ –º–æ–¥–µ–ª–ª—é:</b>\n{}",
        "suggest_models": "üè∑ <b>–†–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω—ñ –º–æ–¥–µ–ª—ñ:</b>\n{}",
        "hf_models": (  
    "üìå <code>HuggingFaceH4/zephyr-7b-beta </code>(7B –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤, ~5GB)\n"  
    "üìå <code>mistralai/Mistral-7B-Instruct-v0.2 </code>(7B, ~5GB)\n"  
    "üìå <code>tiiuae/falcon-7b-instruct </code>(7B, ~5GB)\n"  
      ),
    }
    
    strings_ru = {
        "name": "HuggingChat",
        "no_args": "‚ùå <b>–í–≤–µ–¥–∏—Ç–µ —Å–≤–æ–π –∑–∞–ø—Ä–æ—Å:</b> üìå <code>{}{} {}</code>",
        "no_token": "‚ùå <b>–¢–æ–∫–µ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω! –î–æ–¥–∞–π—Ç–µ:</b> üìå <code>{}cfg huggingchat</code>",
        "asking": "üîÑ <b>–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–æ–≤...</b>",
        "answer": "ü§ñ <b>–û—Ç–≤–µ—Ç {}:</b>\n{}\n\nüí¨ <b>–ó–∞–ø–∏—Ç:</b> {}",
        "api_error": "üö® <b>–û—à–∏–±–∫–∞ API:</b> {}",
        "model_error": "‚ö†Ô∏è <b>–ü—Ä–æ–±–ª–µ–º–∞ –º–æ–¥–µ–ª–∏:</b>\n{}",
        "suggest_models": "üè∑ <b>–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –º–æ–¥–µ–ª–∏:</b>\n{}",
    }

    def __init__(self):
        self.config = loader.ModuleConfig(
            loader.ConfigValue(
                "hf_api_key",
                None,
                "üîë –¢–æ–∫–µ–Ω HuggingFace (WRITE access required)",
                validator=loader.validators.Hidden(loader.validators.String())
            ),
            loader.ConfigValue(
                "default_model",
                "HuggingFaceH4/zephyr-7b-beta",  # –õ–µ–≥–∫–∞ –º–æ–¥–µ–ª—å <10GB
                "ü¶æ Default model (HuggingFace ID)",
            ),
            loader.ConfigValue(
                "max_new_tokens",
                512,  # –ó–º–µ–Ω—à–µ–Ω–æ –¥–ª—è –±–µ–∑–ø–µ–∫–∏
                "üìè Max. Tokens",
                validator=loader.validators.Integer(minimum=100, maximum=1024)
            ),
            loader.ConfigValue(
                "temperature",
                0.7,
                "üé® Creativity (0.1-1.0)",
                validator=loader.validators.Float(minimum=0.1, maximum=1.0)
            ),
        )

    async def client_ready(self, client, db):
        self.client = client
        self.db = db

    def _format_prompt(self, model: str, question: str) -> str:
        """–§–æ—Ä–º–∞—Ç—É–≤–∞–Ω–Ω—è –∑–∞–ø–∏—Ç—É –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–æ –¥–æ –≤–∏–º–æ–≥ –º–æ–¥–µ–ª—ñ"""
        if "zephyr" in model.lower():
            return f"<|user|>\n{question}</s>\n<|assistant|>"
        elif "mistral" in model.lower():
            return f"[INST] {question} [/INST]"
        elif "flan" in model.lower():
            return f"Question: {question}\nAnswer:"
        elif "oasst" in model.lower():
            return f"<|prompter|>{question}<|endoftext|><|assistant|>"
        else:
            return question  # –£–Ω—ñ–≤–µ—Ä—Å–∞–ª—å–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç

    @loader.command(ru_doc="–ú–æ–¥–µ–ª–∏ –∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é", en_doc="Models I recommend")
    async def hfmodels(self, message):
        """–ü–æ–∫–∞–∑–∞—Ç–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω—ñ –º–æ–¥–µ–ª—ñ"""
        await utils.answer(message, self.strings["hf_models"])

    @loader.command()
    async def hfchat(self, message):
        """–ó–∞–¥–∞—Ç–∏ –ø–∏—Ç–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ"""
        question = utils.get_args_raw(message)
        if not question:
            return await utils.answer(message, self.strings["no_args"].format(
                self.get_prefix(), "hfchat", "[–∑–∞–ø–∏—Ç]"
            ))

        if not self.config["hf_api_key"]:
            return await utils.answer(message, self.strings["no_token"].format(self.get_prefix()))

        await utils.answer(message, self.strings["asking"])

        headers = {
            "Authorization": f"Bearer {self.config['hf_api_key']}",
            "Content-Type": "application/json",
        }

        current_model = self.config["default_model"]
        payload = {
            "inputs": self._format_prompt(current_model, question),
            "parameters": {
                "max_new_tokens": self.config["max_new_tokens"],
                "temperature": self.config["temperature"],
                "return_full_text": False
            }
        }

        try:
            response = await utils.run_sync(
                requests.post,
                url=f"https://api-inference.huggingface.co/models/{current_model}",
                headers=headers,
                json=payload
            )

            if response.status_code == 200:
                result = response.json()
                if isinstance(result, list) and "generated_text" in result[0]:
                    answer = result[0]["generated_text"].strip()
                    return await utils.answer(
                        message,
                        self.strings["answer"].format(
                            current_model.split('/')[-1],
                            answer,
                            question
                        )
                    )
                return await utils.answer(message, self.strings["api_error"].format("–ù–µ–≤—ñ—Ä–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ"))

            error_data = response.json()
            error_msg = f"{response.status_code} - {error_data.get('error', '–ù–µ–≤—ñ–¥–æ–º–∞ –ø–æ–º–∏–ª–∫–∞')}"
            
            # –Ø–∫—â–æ –º–æ–¥–µ–ª—å –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—î—Ç—å—Å—è
            if "currently loading" in error_msg.lower():
                suggestions = "\n".join([f"‚Ä¢ {m}" for m in self.recommended_models])
                error_msg += self.strings["suggest_models"].format(suggestions)
            
            return await utils.answer(
                message,
                self.strings["model_error"].format(error_msg)
            )

        except Exception as e:
            logger.exception("API error")
            return await utils.answer(
                message,
                self.strings["api_error"].format(str(e))
            )